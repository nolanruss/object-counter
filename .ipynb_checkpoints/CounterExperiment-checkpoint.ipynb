{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../keras/keras/applications/vgg16.py\n",
    "\"\"\"VGG16 model for Keras.\n",
    "\n",
    "# Reference\n",
    "\n",
    "- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "\n",
    "from ..models import Model\n",
    "from ..layers import Flatten\n",
    "from ..layers import Dense\n",
    "from ..layers import Input\n",
    "from ..layers import Conv2D\n",
    "from ..layers import MaxPooling2D\n",
    "from ..layers import GlobalAveragePooling2D\n",
    "from ..layers import GlobalMaxPooling2D\n",
    "from ..engine.topology import get_source_inputs\n",
    "from ..utils import layer_utils\n",
    "from ..utils.data_utils import get_file\n",
    "from .. import backend as K\n",
    "from .imagenet_utils import decode_predictions\n",
    "from .imagenet_utils import preprocess_input\n",
    "from .imagenet_utils import _obtain_input_shape\n",
    "\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "def VGG16(include_top=True, weights='imagenet',\n",
    "          input_tensor=None, input_shape=None,\n",
    "          pooling=None,\n",
    "          classes=1000):\n",
    "    \"\"\"Instantiates the VGG16 architecture.\n",
    "\n",
    "    Optionally loads weights pre-trained\n",
    "    on ImageNet. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_data_format='channels_last'` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The data format\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or 'imagenet' (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 input channels,\n",
    "            and width and height should be no smaller than 48.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    max4 = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(max4)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    # Block 4 classifier convolution\n",
    "    \n",
    "    # Block 5 classifier convolution\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='vgg16')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                    WEIGHTS_PATH,\n",
    "                                    cache_subdir='models',\n",
    "                                    file_hash='64373286793e3c8b2b4e3219cbf3544b')\n",
    "        else:\n",
    "            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    WEIGHTS_PATH_NO_TOP,\n",
    "                                    cache_subdir='models',\n",
    "                                    file_hash='6d6bbae143d832006294945121d1f1fc')\n",
    "        model.load_weights(weights_path)\n",
    "        if K.backend() == 'theano':\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if include_top:\n",
    "                maxpool = model.get_layer(name='block5_pool')\n",
    "                shape = maxpool.output_shape[1:]\n",
    "                dense = model.get_layer(name='fc1')\n",
    "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
    "\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Add\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Input, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FCN input tensor for the new top\n",
    "fcn_input = Input(shape=(None,None,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convolutional layer for the new top\n",
    "x = Conv2D(1, (1, 1), activation='relu', padding='same', name='block5_conv3')(fcn_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the top into a model\n",
    "fcn_base = Model(inputs=fcn_input, outputs=x, name='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcn_base = Model(inputs=vgg.input, outputs=fcn_base(vgg.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_213 (InputLayer)       (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 5,899,776\n",
      "Trainable params: 5,899,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.get_layer('block4').summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the FCN base blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn_block(nb_filters, block_nb, input_shape=None):\n",
    "    inpt = Input(shape=input_shape)\n",
    "    x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same', name='block'+str(block_nb)+'_conv1')(inpt)\n",
    "    x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same', name='block'+str(block_nb)+'_conv2')(x)\n",
    "    if block_nb > 2:\n",
    "        x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same', name='block'+str(block_nb)+'_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block'+str(block_nb)+'_pool')(x)\n",
    "    model = Model(inputs=inpt, outputs=x, name='block'+str(block_nb))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_fcn_weights(model, wts_model=None):\n",
    "    if wts_model == None:\n",
    "        wts_model = keras.applications.vgg16.VGG16(weights='imagenet')\n",
    "    weights = wts_model.get_weights()\n",
    "    for lyr0 in vgg.layers:\n",
    "        for lyr1 in model.layers:\n",
    "            try:\n",
    "                for lyr2 in lyr1.layers:\n",
    "                    ld_weights(lyr2, lyr0)\n",
    "                    #print(lyr2.name)\n",
    "            except:\n",
    "                ld_weights(lyr1, lyr0)\n",
    "                #print(lyr1.name)\n",
    "def ld_weights(l1, l2):\n",
    "    if(l1.name == l2.name):\n",
    "        l1.set_weights(l2.get_weights())\n",
    "        print(l1.name, 'weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1 weights loaded\n",
      "block1_conv2 weights loaded\n",
      "block1_pool weights loaded\n",
      "block2_conv1 weights loaded\n",
      "block2_conv2 weights loaded\n",
      "block2_pool weights loaded\n",
      "block3_conv1 weights loaded\n",
      "block3_conv2 weights loaded\n",
      "block3_conv3 weights loaded\n",
      "block3_pool weights loaded\n",
      "block4_conv1 weights loaded\n",
      "block4_conv2 weights loaded\n",
      "block4_conv3 weights loaded\n",
      "block4_pool weights loaded\n",
      "block5_conv1 weights loaded\n",
      "block5_conv2 weights loaded\n",
      "block5_conv3 weights loaded\n",
      "block5_pool weights loaded\n"
     ]
    }
   ],
   "source": [
    "load_fcn_weights(mod, vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_block(nb_classes, block_nb, factor=0, input_shape=None):\n",
    "    inpt = Input(shape=input_shape)\n",
    "    x = Conv2D(nb_classes, (1, 1), activation='relu', padding='same', name='block'+str(block_nb)+'upsample_conv1')(inpt)\n",
    "    if factor > 0:\n",
    "        x = UpSampling2D(size=(factor, factor), name='block'+str(block_nb)+'_upsample2D')(x)\n",
    "    model = Model(inputs=inpt, outputs=x, name='block'+str(block_nb)+'_upsample')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upsample_filt(size):\n",
    "    factor = (size + 1) // 2\n",
    "    if size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:size, :size]\n",
    "    return (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
    "\n",
    "def bilinear_upsample_weights(factor, number_of_classes):\n",
    "    filter_size = factor*2 - factor%2\n",
    "    weights = np.zeros((filter_size, filter_size, number_of_classes, number_of_classes),\n",
    "                       dtype=np.float32)\n",
    "    upsample_kernel = upsample_filt(filter_size)\n",
    "    for i in range(number_of_classes):\n",
    "        weights[:, :, i, i] = upsample_kernel\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fcn(copy_model=None):\n",
    "    inpt = Input(shape=(None, None,3))\n",
    "    x = fcn_block(64, 1, input_shape=(None,None,3))(inpt)\n",
    "    x = fcn_block(128, 2, input_shape=(None,None,64))(x)\n",
    "    x = fcn_block(256, 3, input_shape=(None,None,128))(x)\n",
    "    x = fcn_block(512, 4, input_shape=(None,None,256))(x)\n",
    "    x_fork = x\n",
    "    upsamp = upsample_block(input_shape=(None,None,512), nb_classes=1, block_nb=4, factor=0)\n",
    "    x_fork = upsamp(x_fork)\n",
    "    x = fcn_block(512, 5, input_shape=(None,None,512))(x)\n",
    "    x = upsample_block(1, 5, factor=2, input_shape=(None, None, 512))(x)\n",
    "    x = Add(name='')([x, x_fork])\n",
    "    x = Conv2DTranspose(filters=1, \n",
    "                    kernel_size=(32, 32),\n",
    "                    strides=(16, 16),\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer=Constant(bilinear_upsample_weights(factor=16, number_of_classes=1)))(x)\n",
    "    model = Model(inputs=inpt, outputs=x)\n",
    "    load_fcn_weights(model, copy_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1 weights loaded\n",
      "block1_conv2 weights loaded\n",
      "block1_pool weights loaded\n",
      "block2_conv1 weights loaded\n",
      "block2_conv2 weights loaded\n",
      "block2_pool weights loaded\n",
      "block3_conv1 weights loaded\n",
      "block3_conv2 weights loaded\n",
      "block3_conv3 weights loaded\n",
      "block3_pool weights loaded\n",
      "block4_conv1 weights loaded\n",
      "block4_conv2 weights loaded\n",
      "block4_conv3 weights loaded\n",
      "block4_pool weights loaded\n",
      "block5_conv1 weights loaded\n",
      "block5_conv2 weights loaded\n",
      "block5_conv3 weights loaded\n",
      "block5_pool weights loaded\n"
     ]
    }
   ],
   "source": [
    "mod = build_fcn(copy_model=vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_294 (InputLayer)           (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1 (Model)                   (None, None, None, 64 38720       input_294[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block2 (Model)                   (None, None, None, 12 221440      block1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block3 (Model)                   (None, None, None, 25 1475328     block2[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4 (Model)                   (None, None, None, 51 5899776     block3[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block5 (Model)                   (None, None, None, 51 7079424     block4[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block5_upsample (Model)          (None, None, None, 1) 513         block5[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4_upsample (Model)          (None, None, None, 1) 513         block4[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_30 (Add)                     (None, None, None, 1) 0           block5_upsample[1][0]            \n",
      "                                                                   block4_upsample[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DTrans (None, None, None, 1) 1025        add_30[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 14,716,739\n",
      "Trainable params: 14,716,739\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "inpt = Input(shape=(None, None,3))\n",
    "x = fcn_block(64, 1, input_shape=(None,None,3))(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = fcn_block(128, 2, input_shape=(None,None,64))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = fcn_block(256, 3, input_shape=(None,None,128))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = fcn_block(512, 4, input_shape=(None,None,256))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fork a layer for the addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool4_fork = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool4_upsamp = upsample_block(input_shape=(None,None,512), nb_classes=1, block_nb=4, factor=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upped = pool4_upsamp(pool4_fork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another fcn block to model x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = fcn_block(512, 5, input_shape=(None,None,512))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the 1x1 convolution to the final fcn block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = upsample_block(1, 5, factor=2, input_shape=(None, None, 512))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the forked layer and the main output layer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Add(name='')([x, upped])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a transpose layer to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Conv2DTranspose(filters=1, \n",
    "                    kernel_size=(32, 32),\n",
    "                    strides=(16, 16),\n",
    "                    padding='same',\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer=Constant(bilinear_upsample_weights(factor=16, number_of_classes=1)))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod = Model(inputs=inpt, outputs=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.load_weights('vgg16.hdf5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_220 (InputLayer)           (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1 (Model)                   (None, None, None, 64 38720       input_220[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "block2 (Model)                   (None, None, None, 12 221440      block1[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block3 (Model)                   (None, None, None, 25 1475328     block2[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4 (Model)                   (None, None, None, 51 5899776     block3[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block5 (Model)                   (None, None, None, 51 7079424     block4[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block5_upsample (Model)          (None, None, None, 1) 513         block5[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "block4_upsample (Model)          (None, None, None, 1) 513         block4[1][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "add_21 (Add)                     (None, None, None, 1) 0           block5_upsample[1][0]            \n",
      "                                                                   block4_upsample[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DTrans (None, None, None, 1) 1025        add_21[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 14,716,739\n",
      "Trainable params: 14,716,739\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mod.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 1)"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e109cfe4a8>"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAAD8CAYAAAChF5zCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e109ec5b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, a = plt.subplots(2)\n",
    "a[0].imshow(pred[0,:,:,0], interpolation='none')\n",
    "a[1].imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "gt = img[:,:,:,0]\n",
    "gt = np.expand_dims(gt, axis=3)\n",
    "print(gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (1, 224, 224, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils.np_utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-683-8db8c27a1854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1359\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1247\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[0;32m   1248\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1249\u001b[1;33m                                              self._feed_output_shapes)\n\u001b[0m\u001b[0;32m   1250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_check_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 raise ValueError(\n\u001b[0;32m    272\u001b[0m                     \u001b[1;34m'You are passing a target array of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m                     \u001b[1;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m                     \u001b[1;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m                     \u001b[1;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are passing a target array of shape (1, 224, 224, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils.np_utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "mod.fit(batch_size=1, epochs=5,x=img, y=gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inpt, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('CellCounter/img/SIMCEPImages_A24_C100_F1_s25_w1.TIF').resize((224,224))\n",
    "img = img.convert('RGB')\n",
    "img = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32, 43, 512)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
